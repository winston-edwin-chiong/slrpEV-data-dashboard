{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Playground"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal: Create an ML pipeline for each dataframe.\n",
    "\n",
    "- 5-Min: If it's not too slow, maybe(?) I can use k-NN. \n",
    "- Hourly: k-NN. \n",
    "- Daily: ARIMA.\n",
    "- Montly: Average(?, fuck lol idk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>avg_power_demand_W</th>\n",
       "      <th>energy_demand_kWh</th>\n",
       "      <th>peak_power_W</th>\n",
       "      <th>day</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-11-05 10:00:00</th>\n",
       "      <td>6335.000000</td>\n",
       "      <td>3.167500</td>\n",
       "      <td>6335.0</td>\n",
       "      <td>Thursday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-11-05 11:00:00</th>\n",
       "      <td>527.916667</td>\n",
       "      <td>0.527917</td>\n",
       "      <td>6335.0</td>\n",
       "      <td>Thursday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-11-05 12:00:00</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Thursday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-11-05 13:00:00</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Thursday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-11-05 14:00:00</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Thursday</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     avg_power_demand_W  energy_demand_kWh  peak_power_W  \\\n",
       "time                                                                       \n",
       "2020-11-05 10:00:00         6335.000000           3.167500        6335.0   \n",
       "2020-11-05 11:00:00          527.916667           0.527917        6335.0   \n",
       "2020-11-05 12:00:00            0.000000           0.000000           0.0   \n",
       "2020-11-05 13:00:00            0.000000           0.000000           0.0   \n",
       "2020-11-05 14:00:00            0.000000           0.000000           0.0   \n",
       "\n",
       "                          day  \n",
       "time                           \n",
       "2020-11-05 10:00:00  Thursday  \n",
       "2020-11-05 11:00:00  Thursday  \n",
       "2020-11-05 12:00:00  Thursday  \n",
       "2020-11-05 13:00:00  Thursday  \n",
       "2020-11-05 14:00:00  Thursday  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/hourlydemand.csv\")\n",
    "df.set_index(\"time\", inplace=True)\n",
    "df.index = pd.to_datetime(df.index)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "class kNNCrossValidator:\n",
    "\n",
    "    def __init__(self, max_depth, max_neighbors, df:pd.DataFrame, granularity:int):\n",
    "        self.max_depth = max_depth\n",
    "        self.max_neighbors = max_neighbors\n",
    "        self.granularity = granularity\n",
    "        self.__kNN_df = self.__create_df(df)\n",
    "        self.best_params = {\n",
    "            \"best_depth\":None,\n",
    "            \"best_n_neighbors\":None\n",
    "        }\n",
    "\n",
    "    def __create_df(self, df):\n",
    "        df = self.__create_lag_features(df, self.max_depth, self.granularity)\n",
    "        return df\n",
    "\n",
    "    def __X_y_split(self):\n",
    "        X = self.__kNN_df.drop(\n",
    "                columns=[\n",
    "                    self.__kNN_df.columns[0] \n",
    "                ]\n",
    "            ) \n",
    "        y = self.__kNN_df[[self.__kNN_df.columns[0]]]\n",
    "        return X, y\n",
    "\n",
    "    def cross_validate(self):\n",
    "        X, y = self.__X_y_split()\n",
    "        pipeline = Pipeline(\n",
    "            [\n",
    "                (\"subset_features\", SubsetLags()),\n",
    "                (\"estimator\", KNeighborsRegressor())\n",
    "            ]\n",
    "        )\n",
    "        params = {\"estimator__n_neighbors\":np.arange(1,self.max_neighbors+1), \"subset_features__num_lags\":np.arange(1,self.max_depth+1)}\n",
    "        grid = GridSearchCV(\n",
    "            estimator=pipeline,\n",
    "            param_grid=params,\n",
    "            scoring=\"neg_mean_squared_error\",\n",
    "            n_jobs=6,\n",
    "            verbose=0,\n",
    "            cv=[(np.arange(0,int(0.8*len(self.__kNN_df))), np.arange(int(0.8*len(self.__kNN_df)), len(self.__kNN_df)))]\n",
    "        )\n",
    "        grid.fit(X, y)\n",
    "        self.best_params[\"best_depth\"] = grid.best_params_[\"estimator__n_neighbors\"]\n",
    "        self.best_params[\"best_n_neighbors\"] = grid.best_params_[\"subset_features__num_lags\"]\n",
    "\n",
    "    def get_params(self):\n",
    "        \"\"\"\n",
    "        Return order: best depth, best # of neighbors\n",
    "        \"\"\"\n",
    "        return self.best_params[\"best_depth\"], self.best_params[\"best_n_neighbors\"]\n",
    "\n",
    "    def __create_lag_features(self, df, num_lag_depths, num_pts_per_day):\n",
    "        \"\"\"Function takes in a dataframe at a below daily granularity, and returns a \n",
    "        new dataframe with lagged values; a \"lag1\" column will have the previous day's power demand at the same time, \n",
    "        a \"lag2\" column will have two days ago's power demand at the same time, etc. Function will drop rows with \n",
    "        any 'NaN' vlaues. \n",
    "        ~~~\n",
    "        Parameters:\n",
    "        df: Dataframe to append features.\n",
    "        num_lag_depths: Number of lagged features to create.\n",
    "        num_pts_per_day: Number of data points per day. For example, an dataframe with one data point per hour will have one a parameter value of 24. \n",
    "        \"\"\"\n",
    "        \n",
    "        with_lags_df = df.copy(deep=True)\n",
    "        for lag_depth in np.arange(1,num_lag_depths+1):\n",
    "            column = with_lags_df[df.columns[0]].shift(num_pts_per_day*lag_depth)\n",
    "            with_lags_df = pd.concat([with_lags_df, column.rename(\"lag\" + f\"{lag_depth}\")], axis=1)\n",
    "        return with_lags_df.dropna()\n",
    "\n",
    "    def __prepare_TS(self, df):\n",
    "        \"\"\"\n",
    "        Function takes in a dataframe and sets the index to the \"time\" column, \n",
    "        changes that index to a pd.datetime-like object.\n",
    "        \"\"\"\n",
    "        df.set_index(\"time\", inplace=True)\n",
    "        df.index = pd.to_datetime(df.index)\n",
    "        if len(df.columns == 1):\n",
    "            raise Exception(\"There should only be a prediction quantity. Dataframe must be Nx1!\")\n",
    "        return df\n",
    "\n",
    "class SubsetLags(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, num_lags=1):\n",
    "        super().__init__()\n",
    "        self.num_lags = num_lags\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return self.__select_subset_lags(X, self.num_lags)\n",
    "        \n",
    "    def __select_subset_lags(self, X, num_features):\n",
    "        \"\"\"Function takes in a dataframe with a 'energy_demand_kWh' column at select granularity with \"lag1\", \"lag2\",..., \"lagN\" features. \n",
    "        Function will select the first \"num_features\" of lags.\"\"\"\n",
    "        return X[[f\"lag{depth}\" for depth in np.arange(1, num_features+1)]]\n",
    "\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "\n",
    "class GenerateOneHourlyDayAheadForecast(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, granularity=24, best_depth=None, best_neighbors=None, max_depth=60, max_neighbors=20,) -> None:\n",
    "        self.max_depth = max_depth\n",
    "        self.max_neighbors = max_neighbors\n",
    "        self.best_depth = best_depth\n",
    "        self.best_neighbors = best_neighbors\n",
    "        self.granularity = granularity \n",
    "        super().__init__()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "\n",
    "        # X = self.__prepare_TS(X) # Can be another step in the pipeline? \n",
    "\n",
    "        if self.best_depth == None and self.best_neighbors == None:\n",
    "            # cross validate \n",
    "            CV = kNNCrossValidator(\n",
    "                max_depth=self.max_depth,\n",
    "                max_neighbors=self.max_neighbors,\n",
    "                df=X,\n",
    "                granularity=self.granularity\n",
    "            )\n",
    "            CV.cross_validate()\n",
    "            self.best_depth, self.best_neighbors = CV.get_params()\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = self.__append_forecast_index(X)\n",
    "        # add features to dataframe\n",
    "        X = self.__create_lag_features(\n",
    "            df=X, \n",
    "            num_lag_depths=self.best_depth, \n",
    "            col_name=X.columns[0],\n",
    "            num_pts_per_day=self.granularity,\n",
    "            )\n",
    "        # create estimator \n",
    "        estimator = KNeighborsRegressor(\n",
    "            n_neighbors = self.best_neighbors,\n",
    "            n_jobs=8\n",
    "            )\n",
    "        # train test split \n",
    "        X_train, X_test, y_train, y_test = self.__X_y_split(X)\n",
    "        estimator.fit(X_train, y_train)\n",
    "        \n",
    "        return pd.DataFrame(index=X_test.index, data={f\"{X.columns[0]} forecast\":estimator.predict(X_test).reshape(-1)})\n",
    "    \n",
    "    def __X_y_split(self, df, test_days=7):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            df.drop(columns=[df.columns[0]]),\n",
    "            df[[df.columns[0]]],\n",
    "            test_size = self.granularity*test_days, # last week of data\n",
    "            shuffle=False\n",
    "        )\n",
    "        return X_train, X_test, y_train, y_test\n",
    "\n",
    "    def __append_forecast_index(self, df, extra_days=1):\n",
    "        \"\"\"\n",
    "        Adds extra datetime indicies at the end of a time series dataframe. \n",
    "        \"\"\"\n",
    "        appendage = pd.DataFrame(index=df.tail(24).index + pd.Timedelta(days=extra_days))\n",
    "        return pd.concat([df, appendage])\n",
    "    \n",
    "    def __create_lag_features(self, df, num_lag_depths, col_name, num_pts_per_day):\n",
    "        \"\"\"\n",
    "        Function takes in a dataframe at a below daily granularity, and returns a \n",
    "        new dataframe with lagged values; a \"lag1\" column will have the previous day's power demand at the same time, \n",
    "        a \"lag2\" column will have two days ago's power demand at the same time, etc. Function will drop rows with \n",
    "        any 'NaN' vlaues. \n",
    "        ~~~\n",
    "        Parameters:\n",
    "        df: Dataframe to append features.\n",
    "        col_name: Column to use.\n",
    "        num_lag_depths: Number of lagged features to create.\n",
    "        num_pts_per_day: Number of data points per day. For example, an dataframe with one data point per hour will have one a parameter value of 24. \n",
    "        \"\"\"\n",
    "        \n",
    "        with_lags_df = df.copy(deep=True)\n",
    "        for lag_depth in np.arange(1,num_lag_depths+1):\n",
    "            column = with_lags_df[col_name].shift(num_pts_per_day*lag_depth)\n",
    "            with_lags_df = pd.concat([with_lags_df, column.rename(\"lag\" + f\"{lag_depth}\")], axis=1)\n",
    "        return with_lags_df.dropna(subset=with_lags_df.columns.drop(col_name))\n",
    "\n",
    "    def __prepare_TS(self, df):\n",
    "        \"\"\"\n",
    "        Function takes in a dataframe and sets the index to the \"time\" column, \n",
    "        changes that index to a pd.datetime-like object.\n",
    "        \"\"\"\n",
    "        df.set_index(\"time\", inplace=True)\n",
    "        df.index = pd.to_datetime(df.index)\n",
    "        return df\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "class GenerateHourlyForecast():\n",
    "\n",
    "    def __init__(self):\n",
    "        self.__add_pipeline()\n",
    "        self.data = self.__prepare_TS(pd.read_csv(\"data/hourlydemand.csv\"))\n",
    "        self.columns = self.data.columns\n",
    "\n",
    "    def __add_pipeline(self):\n",
    "        self.__pipeline = Pipeline(\n",
    "            [\n",
    "                (\"generate_one_forecast\", GenerateOneHourlyDayAheadForecast(best_depth=60, best_neighbors=20))\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def generate_forecasts(self):\n",
    "        for col_name in self.columns:\n",
    "            data = self.data[[col_name]]\n",
    "            predictions = self.__pipeline.fit_transform(data)\n",
    "            predictions.to_csv(f\"forecasts/data/{col_name}_forecast.csv\")   \n",
    "\n",
    "    def __prepare_TS(self, df):\n",
    "        \"\"\"\n",
    "        Function takes in a dataframe and sets the index to the \"time\" column, \n",
    "        changes that index to a pd.datetime-like object.\n",
    "        \"\"\"\n",
    "        df.set_index(\"time\", inplace=True)\n",
    "        df.index = pd.to_datetime(df.index)\n",
    "        df.columns.drop(\"day\")\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'Sunday'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Winston\\Desktop\\slrpEV-data-dashboard\\Playground.ipynb Cell 6\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Winston/Desktop/slrpEV-data-dashboard/Playground.ipynb#X33sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m GenerateHourlyForecast()\u001b[39m.\u001b[39;49mgenerate_forecasts()\n",
      "\u001b[1;32mc:\\Users\\Winston\\Desktop\\slrpEV-data-dashboard\\Playground.ipynb Cell 6\u001b[0m in \u001b[0;36mGenerateHourlyForecast.generate_forecasts\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Winston/Desktop/slrpEV-data-dashboard/Playground.ipynb#X33sZmlsZQ%3D%3D?line=223'>224</a>\u001b[0m \u001b[39mfor\u001b[39;00m col_name \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns:\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Winston/Desktop/slrpEV-data-dashboard/Playground.ipynb#X33sZmlsZQ%3D%3D?line=224'>225</a>\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata[[col_name]]\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/Winston/Desktop/slrpEV-data-dashboard/Playground.ipynb#X33sZmlsZQ%3D%3D?line=225'>226</a>\u001b[0m     predictions \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__pipeline\u001b[39m.\u001b[39;49mfit_transform(data)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Winston/Desktop/slrpEV-data-dashboard/Playground.ipynb#X33sZmlsZQ%3D%3D?line=226'>227</a>\u001b[0m     predictions\u001b[39m.\u001b[39mto_csv(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mforecasts/data/\u001b[39m\u001b[39m{\u001b[39;00mcol_name\u001b[39m}\u001b[39;00m\u001b[39m_forecast.csv\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Winston\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py:434\u001b[0m, in \u001b[0;36mPipeline.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    432\u001b[0m fit_params_last_step \u001b[39m=\u001b[39m fit_params_steps[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m0\u001b[39m]]\n\u001b[0;32m    433\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(last_step, \u001b[39m\"\u001b[39m\u001b[39mfit_transform\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m--> 434\u001b[0m     \u001b[39mreturn\u001b[39;00m last_step\u001b[39m.\u001b[39mfit_transform(Xt, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params_last_step)\n\u001b[0;32m    435\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    436\u001b[0m     \u001b[39mreturn\u001b[39;00m last_step\u001b[39m.\u001b[39mfit(Xt, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params_last_step)\u001b[39m.\u001b[39mtransform(Xt)\n",
      "File \u001b[1;32mc:\\Users\\Winston\\anaconda3\\lib\\site-packages\\sklearn\\base.py:852\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    848\u001b[0m \u001b[39m# non-optimized default implementation; override when a better\u001b[39;00m\n\u001b[0;32m    849\u001b[0m \u001b[39m# method is possible for a given clustering algorithm\u001b[39;00m\n\u001b[0;32m    850\u001b[0m \u001b[39mif\u001b[39;00m y \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    851\u001b[0m     \u001b[39m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[1;32m--> 852\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit(X, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params)\u001b[39m.\u001b[39;49mtransform(X)\n\u001b[0;32m    853\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    854\u001b[0m     \u001b[39m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[0;32m    855\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\u001b[39m.\u001b[39mtransform(X)\n",
      "\u001b[1;32mc:\\Users\\Winston\\Desktop\\slrpEV-data-dashboard\\Playground.ipynb Cell 6\u001b[0m in \u001b[0;36mGenerateOneHourlyDayAheadForecast.transform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Winston/Desktop/slrpEV-data-dashboard/Playground.ipynb#X33sZmlsZQ%3D%3D?line=153'>154</a>\u001b[0m \u001b[39m# train test split \u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Winston/Desktop/slrpEV-data-dashboard/Playground.ipynb#X33sZmlsZQ%3D%3D?line=154'>155</a>\u001b[0m X_train, X_test, y_train, y_test \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__X_y_split(X)\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/Winston/Desktop/slrpEV-data-dashboard/Playground.ipynb#X33sZmlsZQ%3D%3D?line=155'>156</a>\u001b[0m estimator\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Winston/Desktop/slrpEV-data-dashboard/Playground.ipynb#X33sZmlsZQ%3D%3D?line=157'>158</a>\u001b[0m \u001b[39mreturn\u001b[39;00m pd\u001b[39m.\u001b[39mDataFrame(index\u001b[39m=\u001b[39mX_test\u001b[39m.\u001b[39mindex, data\u001b[39m=\u001b[39m{\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mX\u001b[39m.\u001b[39mcolumns[\u001b[39m0\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m forecast\u001b[39m\u001b[39m\"\u001b[39m:estimator\u001b[39m.\u001b[39mpredict(X_test)\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)})\n",
      "File \u001b[1;32mc:\\Users\\Winston\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_regression.py:213\u001b[0m, in \u001b[0;36mKNeighborsRegressor.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[39m\"\"\"Fit the k-nearest neighbors regressor from the training dataset.\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \n\u001b[0;32m    196\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[39m    The fitted k-nearest neighbors regressor.\u001b[39;00m\n\u001b[0;32m    210\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    211\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweights \u001b[39m=\u001b[39m _check_weights(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweights)\n\u001b[1;32m--> 213\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(X, y)\n",
      "File \u001b[1;32mc:\\Users\\Winston\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_base.py:400\u001b[0m, in \u001b[0;36mNeighborsBase._fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    398\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_tags()[\u001b[39m\"\u001b[39m\u001b[39mrequires_y\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[0;32m    399\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(X, (KDTree, BallTree, NeighborsBase)):\n\u001b[1;32m--> 400\u001b[0m         X, y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(X, y, accept_sparse\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m, multi_output\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m    402\u001b[0m     \u001b[39mif\u001b[39;00m is_classifier(\u001b[39mself\u001b[39m):\n\u001b[0;32m    403\u001b[0m         \u001b[39m# Classification targets require a specific format\u001b[39;00m\n\u001b[0;32m    404\u001b[0m         \u001b[39mif\u001b[39;00m y\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39mor\u001b[39;00m y\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m \u001b[39mand\u001b[39;00m y\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Winston\\anaconda3\\lib\\site-packages\\sklearn\\base.py:581\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    579\u001b[0m         y \u001b[39m=\u001b[39m check_array(y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_y_params)\n\u001b[0;32m    580\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 581\u001b[0m         X, y \u001b[39m=\u001b[39m check_X_y(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_params)\n\u001b[0;32m    582\u001b[0m     out \u001b[39m=\u001b[39m X, y\n\u001b[0;32m    584\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m check_params\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mensure_2d\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m):\n",
      "File \u001b[1;32mc:\\Users\\Winston\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:964\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m    961\u001b[0m \u001b[39mif\u001b[39;00m y \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    962\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39my cannot be None\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 964\u001b[0m X \u001b[39m=\u001b[39m check_array(\n\u001b[0;32m    965\u001b[0m     X,\n\u001b[0;32m    966\u001b[0m     accept_sparse\u001b[39m=\u001b[39;49maccept_sparse,\n\u001b[0;32m    967\u001b[0m     accept_large_sparse\u001b[39m=\u001b[39;49maccept_large_sparse,\n\u001b[0;32m    968\u001b[0m     dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[0;32m    969\u001b[0m     order\u001b[39m=\u001b[39;49morder,\n\u001b[0;32m    970\u001b[0m     copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[0;32m    971\u001b[0m     force_all_finite\u001b[39m=\u001b[39;49mforce_all_finite,\n\u001b[0;32m    972\u001b[0m     ensure_2d\u001b[39m=\u001b[39;49mensure_2d,\n\u001b[0;32m    973\u001b[0m     allow_nd\u001b[39m=\u001b[39;49mallow_nd,\n\u001b[0;32m    974\u001b[0m     ensure_min_samples\u001b[39m=\u001b[39;49mensure_min_samples,\n\u001b[0;32m    975\u001b[0m     ensure_min_features\u001b[39m=\u001b[39;49mensure_min_features,\n\u001b[0;32m    976\u001b[0m     estimator\u001b[39m=\u001b[39;49mestimator,\n\u001b[0;32m    977\u001b[0m )\n\u001b[0;32m    979\u001b[0m y \u001b[39m=\u001b[39m _check_y(y, multi_output\u001b[39m=\u001b[39mmulti_output, y_numeric\u001b[39m=\u001b[39my_numeric)\n\u001b[0;32m    981\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[1;32mc:\\Users\\Winston\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:746\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    744\u001b[0m         array \u001b[39m=\u001b[39m array\u001b[39m.\u001b[39mastype(dtype, casting\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39munsafe\u001b[39m\u001b[39m\"\u001b[39m, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    745\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 746\u001b[0m         array \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49masarray(array, order\u001b[39m=\u001b[39;49morder, dtype\u001b[39m=\u001b[39;49mdtype)\n\u001b[0;32m    747\u001b[0m \u001b[39mexcept\u001b[39;00m ComplexWarning \u001b[39mas\u001b[39;00m complex_warning:\n\u001b[0;32m    748\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    749\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mComplex data not supported\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(array)\n\u001b[0;32m    750\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39mcomplex_warning\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Winston\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:2064\u001b[0m, in \u001b[0;36mNDFrame.__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m   2063\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__array__\u001b[39m(\u001b[39mself\u001b[39m, dtype: npt\u001b[39m.\u001b[39mDTypeLike \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m np\u001b[39m.\u001b[39mndarray:\n\u001b[1;32m-> 2064\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39;49masarray(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_values, dtype\u001b[39m=\u001b[39;49mdtype)\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'Sunday'"
     ]
    }
   ],
   "source": [
    "GenerateHourlyForecast().generate_forecasts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "475dd3d8de922d6629668699edf7da91807dd0e731d2ca4abf0ed1b52cb8d54e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
