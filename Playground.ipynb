{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os \n",
    "import pandas as pd\n",
    "import boto3\n",
    "from boto3.dynamodb.conditions import Key, Attr\n",
    "\n",
    "class FetchData:\n",
    "\n",
    "    def __init__(self, table_id):\n",
    "        self.__scan_results = []\n",
    "        self.raw_data = None\n",
    "        self.table_id = table_id\n",
    "        self.__table = self.__get_table(self.table_id)\n",
    "\n",
    "    def __configure(self):\n",
    "        load_dotenv()\n",
    "\n",
    "    def __get_table(self, table_id):\n",
    "        self.__configure()\n",
    "        dynamodb = boto3.resource(\n",
    "            \"dynamodb\",\n",
    "            aws_access_key_id = os.getenv(\"access_key_id\"),\n",
    "            aws_secret_access_key = os.getenv(\"secret_access_key\"),\n",
    "            region_name=\"us-east-2\",\n",
    "        )\n",
    "        return dynamodb.Table(table_id)\n",
    "\n",
    "    # TODO: Only query for records I don't have. Currently don't know how, so I'm scanning for all records.\n",
    "    def scan_save_all_records(self):\n",
    "        done = False\n",
    "        start_key = None\n",
    "        params = {}\n",
    "\n",
    "        while not done:\n",
    "            if start_key is not None:\n",
    "                params = {\"ExclusiveStartKey\": start_key}\n",
    "                \n",
    "            response = self.__table.scan(**params)\n",
    "            self.__scan_results.extend(response.get(\"Items\", []))\n",
    "\n",
    "            start_key = response.get(\"LastEvaluatedKey\", None)\n",
    "            done = start_key is None\n",
    "\n",
    "        self.raw_data = pd.json_normalize(self.__scan_results)\n",
    "        self.raw_data.to_csv(\"data/raw_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CleanData:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.__add_pipeline()\n",
    "        self.__raw_data = pd.read_csv(\"data/raw_data.csv\")\n",
    "\n",
    "    def __add_pipeline(self):\n",
    "        self.__pipeline = Pipeline([\n",
    "            (\"sort_drop_cast\", SortDropCast()),\n",
    "            (\"create_helpers\", HelperFeatureCreation()),\n",
    "            (\"create_session_TS\", CreateSessionTS()),\n",
    "            (\"create_features\", FeatureCreation()),\n",
    "            (\"save_to_csv\", SaveToCsv()),\n",
    "        ])\n",
    "\n",
    "    def clean_raw_data(self):\n",
    "        self.cleaned_data = self.__pipeline.fit_transform(self.__raw_data)\n",
    "        return self.cleaned_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import pandas as pd \n",
    "\n",
    "class SortDropCast(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    This pipeline step will sort values by field \"connectTime\",\n",
    "    drop columns \"user_email\", \"slrpPaymentId\", \n",
    "    and cast columns \"cumEnergy_Wh\", \"peakPower_W\" as float values. \n",
    "    \"\"\"\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X) -> pd.DataFrame:\n",
    "        X = X.sort_values(by=\"connectTime\").drop(columns=[\"user_email\", \"slrpPaymentId\"]).reset_index(drop=True)\n",
    "        X[\"cumEnergy_Wh\"] = X[\"cumEnergy_Wh\"].astype(float)\n",
    "        X[\"peakPower_W\"] = X[\"peakPower_W\"].astype(float)\n",
    "        return X\n",
    "\n",
    "class HelperFeatureCreation(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    This pipeline step will drop any records that contain 0 for \n",
    "    \"peakPower_W\" or \"cumEnergy_Wh\". Two additional columns will be created:\n",
    "    \"reqChargeTime\" and \"finishChargeTime\".\n",
    "    \"\"\"\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X) -> pd.DataFrame:\n",
    "        X = X.loc[(X[\"peakPower_W\"] != 0) & (X[\"cumEnergy_Wh\"] != 0)]\n",
    "        X = X.assign(reqChargeTime_h = (X[\"cumEnergy_Wh\"] / X[\"peakPower_W\"]))\n",
    "        X = X.assign(connectTime = (pd.to_datetime(X[\"connectTime\"])))\n",
    "        X = X.assign(finishChargeTime = (X[\"connectTime\"] + pd.to_timedelta(X['reqChargeTime_h'], unit='hours').round(\"s\")))\n",
    "        return X \n",
    "\n",
    "class CreateSessionTS(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    This pipeline step will create a time series for each session. A dataframe\n",
    "    with 5-min granularity will be returned, with one column, \"power_demand_W\".\n",
    "    \"\"\"\n",
    "    def __createTS(self, session):\n",
    "        \"\"\"\n",
    "        This helper function takes in a session, with a \"connectTime\", \"finishChargeTime\", and \n",
    "        a \"peakPower_W\" column. Function will return a time series at 5-min granularity. \n",
    "        \"\"\"\n",
    "        date_range = pd.date_range(start=session[\"connectTime\"], end=session[\"finishChargeTime\"], freq=\"5min\")\n",
    "        temp_df = pd.DataFrame(index=date_range)\n",
    "        temp_df[\"power_demand_W\"] = session[\"peakPower_W\"] # rename \n",
    "        self.rows.append(temp_df)   \n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self.rows = []\n",
    "        super().__init__()\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self \n",
    "\n",
    "    def transform(self, X) -> pd.DataFrame:\n",
    "        X.apply(self.__createTS, axis=1)\n",
    "        X = pd.concat(self.rows, axis=0).sort_index()\n",
    "        X = X.resample(\"5min\").sum()\n",
    "        return X \n",
    "\n",
    "class FeatureCreation(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    This pipeline step will create an \"energy_demand_kWh\" and \"peak_power_W\" column. \n",
    "    The name of the dataframe's index will be set to \"time\", and a \"day\" column will \n",
    "    be created with the day of the week at each timestamp. \n",
    "    \"\"\"\n",
    "    def fit(self, X, y=None):\n",
    "        return self \n",
    "\n",
    "    def transform(self, X) -> pd.DataFrame:\n",
    "        X[\"energy_demand_kWh\"] = (X[\"power_demand_W\"]/1000)/12\n",
    "        # for the highest granularity, peak power is equal to the power demand (different for different granularities though)\n",
    "        X[\"peak_power_W\"] = X[\"power_demand_W\"] \n",
    "        X.index.name = \"time\"\n",
    "        X[\"day\"] = X.index.day_name()\n",
    "        return X\n",
    "\n",
    "class SaveToCsv(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    This pipeline step takes each dataframe and creates new granularities\n",
    "    (hourly, daily, and monthly). Each dataframe is saved to a \"data/\" file. \n",
    "    \"\"\"\n",
    "    def __init__(self) -> None:\n",
    "        self.agg_key = {\n",
    "            \"power_demand_W\": \"mean\",\n",
    "            \"energy_demand_kWh\": \"sum\",\n",
    "            \"peak_power_W\": \"max\",\n",
    "            \"day\": \"first\"\n",
    "        }\n",
    "        self.dataframe_names = [\n",
    "            \"fivemindemand\", \n",
    "            \"hourlydemand\", \n",
    "            \"dailydemand\", \n",
    "            \"monthlydemand\"\n",
    "        ]\n",
    "        self.new_dataframes = []\n",
    "        super().__init__()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        hourlydemand = X.resample(\"1H\").agg(self.agg_key).rename(columns={\"power_demand_W\":\"avg_power_demand_W\"})\n",
    "        dailydemand = X.resample(\"24H\").agg(self.agg_key).rename(columns={\"power_demand_W\":\"avg_power_demand_W\"})\n",
    "        monthlydemand = X.resample(\"1M\").agg(self.agg_key).rename(columns={\"power_demand_W\":\"avg_power_demand_W\"})\n",
    "        self.new_dataframes.extend([X, hourlydemand, dailydemand, monthlydemand])\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        for idx, dataframe in enumerate(self.new_dataframes):\n",
    "            dataframe.to_csv(f\"data/{self.dataframe_names[idx]}.csv\")\n",
    "        return X "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>power_demand_W</th>\n",
       "      <th>energy_demand_kWh</th>\n",
       "      <th>peak_power_W</th>\n",
       "      <th>day</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-11-05 10:30:00</th>\n",
       "      <td>6335.0</td>\n",
       "      <td>0.527917</td>\n",
       "      <td>6335.0</td>\n",
       "      <td>Thursday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-11-05 10:35:00</th>\n",
       "      <td>6335.0</td>\n",
       "      <td>0.527917</td>\n",
       "      <td>6335.0</td>\n",
       "      <td>Thursday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-11-05 10:40:00</th>\n",
       "      <td>6335.0</td>\n",
       "      <td>0.527917</td>\n",
       "      <td>6335.0</td>\n",
       "      <td>Thursday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-11-05 10:45:00</th>\n",
       "      <td>6335.0</td>\n",
       "      <td>0.527917</td>\n",
       "      <td>6335.0</td>\n",
       "      <td>Thursday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-11-05 10:50:00</th>\n",
       "      <td>6335.0</td>\n",
       "      <td>0.527917</td>\n",
       "      <td>6335.0</td>\n",
       "      <td>Thursday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-07 23:55:00</th>\n",
       "      <td>3333.0</td>\n",
       "      <td>0.277750</td>\n",
       "      <td>3333.0</td>\n",
       "      <td>Saturday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-08 00:00:00</th>\n",
       "      <td>3333.0</td>\n",
       "      <td>0.277750</td>\n",
       "      <td>3333.0</td>\n",
       "      <td>Sunday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-08 00:05:00</th>\n",
       "      <td>3333.0</td>\n",
       "      <td>0.277750</td>\n",
       "      <td>3333.0</td>\n",
       "      <td>Sunday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-08 00:10:00</th>\n",
       "      <td>3333.0</td>\n",
       "      <td>0.277750</td>\n",
       "      <td>3333.0</td>\n",
       "      <td>Sunday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-08 00:15:00</th>\n",
       "      <td>3333.0</td>\n",
       "      <td>0.277750</td>\n",
       "      <td>3333.0</td>\n",
       "      <td>Sunday</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>228550 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     power_demand_W  energy_demand_kWh  peak_power_W       day\n",
       "time                                                                          \n",
       "2020-11-05 10:30:00          6335.0           0.527917        6335.0  Thursday\n",
       "2020-11-05 10:35:00          6335.0           0.527917        6335.0  Thursday\n",
       "2020-11-05 10:40:00          6335.0           0.527917        6335.0  Thursday\n",
       "2020-11-05 10:45:00          6335.0           0.527917        6335.0  Thursday\n",
       "2020-11-05 10:50:00          6335.0           0.527917        6335.0  Thursday\n",
       "...                             ...                ...           ...       ...\n",
       "2023-01-07 23:55:00          3333.0           0.277750        3333.0  Saturday\n",
       "2023-01-08 00:00:00          3333.0           0.277750        3333.0    Sunday\n",
       "2023-01-08 00:05:00          3333.0           0.277750        3333.0    Sunday\n",
       "2023-01-08 00:10:00          3333.0           0.277750        3333.0    Sunday\n",
       "2023-01-08 00:15:00          3333.0           0.277750        3333.0    Sunday\n",
       "\n",
       "[228550 rows x 4 columns]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_grabber = FetchData(\"Sessions2\")\n",
    "data_grabber.scan_save_all_records()\n",
    "data_cleaner = CleanData()\n",
    "cleaned = data_cleaner.clean_raw_data()\n",
    "\n",
    "df = pd.read_csv(\"data/raw_data.csv\")\n",
    "\n",
    "# # pipeline\n",
    "# pipe = Pipeline([\n",
    "#         (\"1\", SortDropCast()),\n",
    "#         (\"2\", HelperFeatureCreation()),\n",
    "#         (\"3\", CreateSessionTS()),\n",
    "#         (\"4\", FeatureCreation()),\n",
    "#         (\"5\", SaveToCsv()),\n",
    "# ])\n",
    "\n",
    "# # preprocess data\n",
    "# cleaned = pipe.fit_transform(df)\n",
    "cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[{'power_W': Decimal('6214'), 'timestamp': Decimal('1672878308')}]\""
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\"data/raw_data.csv\").sort_values(by=\"connectTime\").tail(50)[\"power\"].iloc[-17]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "475dd3d8de922d6629668699edf7da91807dd0e731d2ca4abf0ed1b52cb8d54e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
