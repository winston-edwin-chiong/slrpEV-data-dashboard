{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "092882a5-2bf2-4b69-ac42-0b7e845850c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from sklearn.pipeline import Pipeline\n",
    "import plotly.express as px\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f87b90f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['abc'],\n",
       "       ['abc'],\n",
       "       ['abc'],\n",
       "       ['abc']], dtype='<U3')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.vstack(np.repeat([\"abc\"], 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dea0d6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "\n",
    "## Main Class\n",
    "class HourlyCrossValidator:\n",
    "\n",
    "    columns = [\"energy_demand_kWh\", \"peak_power_W\"]\n",
    "\n",
    "    def __init__(self, max_neighbors, max_depth):\n",
    "        self.max_neighbors = max_neighbors\n",
    "        self.max_depth = max_depth\n",
    "\n",
    "    def cross_validate(self, df):\n",
    "\n",
    "        best_params = {}\n",
    "\n",
    "        for column in self.columns:\n",
    "            # initalize cross validator, cross validate  \n",
    "            params = kNNCrossValidator(self.max_neighbors, self.max_depth, column).cross_validate_one(df)\n",
    "            best_params[column] = params\n",
    "\n",
    "        best_params[\"avg_power_demand_W\"] = best_params[\"energy_demand_kWh\"] # same data, different units, so same parameters\n",
    "\n",
    "        return best_params\n",
    "    \n",
    "\n",
    "## Helper Class\n",
    "class kNNCrossValidator:\n",
    "\n",
    "    test_size = 0.2\n",
    "\n",
    "    def __init__(self, max_neighbors, max_depth, col_name):\n",
    "        super().__init__()\n",
    "        self.max_neighbors = max_neighbors\n",
    "        self.max_depth = max_depth\n",
    "        self.col_name = col_name\n",
    "\n",
    "\n",
    "    def cross_validate_one(self, df) -> dict:\n",
    "        # create features\n",
    "        df = self.__create_all_lag_features(df)\n",
    "\n",
    "        # create validation pipeline\n",
    "        validation_pipeline = Pipeline([\n",
    "            (\"subset_features\", SubsetLags()),\n",
    "            (\"estimator\", KNeighborsRegressor())\n",
    "        ])\n",
    "\n",
    "        # create parameter grid\n",
    "        params = {\n",
    "            \"estimator__n_neighbors\": np.arange(10, self.max_neighbors+1), # start searching at 10 neighbors\n",
    "            \"subset_features__num_lags\": np.arange(40, self.max_depth+1) # start searching at 40 lags as features\n",
    "        }\n",
    "\n",
    "        # split data into train, validation, and test\n",
    "        X_train_validation, X_train, X_validation, X_test, y_train_validation, y_train, y_validation, y_test = self.__train_test_split(df)\n",
    "\n",
    "        # create grid and iteratively search\n",
    "        grid = GridSearchCV(\n",
    "            estimator=validation_pipeline,\n",
    "            param_grid=params,\n",
    "            scoring=\"neg_mean_squared_error\",\n",
    "            n_jobs=8,\n",
    "            verbose=4,\n",
    "            cv=[(np.arange(0, len(X_train)), np.arange(len(X_train), len(X_train_validation)))]\n",
    "        )\n",
    "        grid.fit(X_train_validation, y_train_validation)\n",
    "\n",
    "        best_params = {\n",
    "            \"best_depth\": grid.best_params_[\"subset_features__num_lags\"],\n",
    "            \"best_n_neighbors\": grid.best_params_[\"estimator__n_neighbors\"]\n",
    "        }\n",
    "\n",
    "        return best_params\n",
    "\n",
    "\n",
    "    def __create_all_lag_features(self, df) -> pd.DataFrame:\n",
    "        # create pipeline, pass data, create features\n",
    "        pipeline = Pipeline([\n",
    "            (\"create_features\", CreateLagFeatures(\n",
    "                self.max_depth, self.col_name))\n",
    "        ])\n",
    "        df_with_features = pipeline.fit_transform(df)\n",
    "\n",
    "        return df_with_features\n",
    "\n",
    "\n",
    "    def __train_test_split(self, df):\n",
    "\n",
    "        # split into train+validation and test\n",
    "        X_train_validation, X_test, y_train_validation, y_test = train_test_split(\n",
    "            df.filter(regex=\"lag\"),  # select only \"lag-\" features\n",
    "            df[[self.col_name]],\n",
    "            test_size=self.test_size,\n",
    "            shuffle=False  # time series split\n",
    "        )\n",
    "\n",
    "        # split into train and validation\n",
    "        X_train, X_validation, y_train, y_validation = train_test_split(\n",
    "            X_train_validation,\n",
    "            y_train_validation,\n",
    "            test_size=0.2,\n",
    "            shuffle=False\n",
    "        )\n",
    "\n",
    "        return X_train_validation, X_train, X_validation, X_test, y_train_validation, y_train, y_validation, y_test\n",
    "\n",
    "\n",
    "\n",
    "## Pipeline Classes\n",
    "class CreateLagFeatures(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, num_lag_depths, col_name):\n",
    "        super().__init__()\n",
    "        self.num_lags_depths = num_lag_depths\n",
    "        self.col_name = col_name\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self \n",
    "    \n",
    "    def transform(self, X):\n",
    "        return self.__create_lag_features(X, self.num_lags_depths, self.col_name)\n",
    "    \n",
    "    @staticmethod\n",
    "    def __create_lag_features(df, num_lag_depths, col_name):\n",
    "        df_with_lags = df.copy(deep=True)\n",
    "        for lag_depth in np.arange(1,num_lag_depths+1):\n",
    "            column = df_with_lags[col_name].shift(24*lag_depth)\n",
    "            df_with_lags = pd.concat([df_with_lags, column.rename(\"lag\" + f\"{lag_depth}\")], axis=1)\n",
    "        return df_with_lags.dropna()\n",
    "\n",
    "\n",
    "class SubsetLags(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, num_lags=1):\n",
    "        super().__init__()\n",
    "        self.num_lags = num_lags\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return self.__select_subset_lags(X, self.num_lags)\n",
    "    \n",
    "    @staticmethod\n",
    "    def __select_subset_lags(df, num_lags):\n",
    "        return df[[f\"lag{depth}\" for depth in np.arange(1, num_lags+1)]]\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pmdarima as pm\n",
    "\n",
    "\n",
    "# Main Class\n",
    "class DailyCrossValidator:\n",
    "\n",
    "    columns = [\"energy_demand_kWh\", \"peak_power_W\"]\n",
    "\n",
    "    @classmethod\n",
    "    def cross_validate(cls, df):\n",
    "\n",
    "        best_params = {}\n",
    "\n",
    "        for column in cls.columns:\n",
    "            # initalize cross validator, cross validate\n",
    "            params = SARIMACrossValidator(column).cross_validate_one(df)\n",
    "            best_params[column] = params\n",
    "\n",
    "        # same data, different units, so same parameters\n",
    "        best_params[\"avg_power_demand_W\"] = best_params[\"energy_demand_kWh\"]\n",
    "\n",
    "        return best_params\n",
    "\n",
    "\n",
    "# Helper Class\n",
    "class SARIMACrossValidator:\n",
    "\n",
    "    test_size = 0.2\n",
    "\n",
    "    def __init__(self, col_name):\n",
    "        super().__init__()\n",
    "        self.col_name = col_name\n",
    "\n",
    "    def cross_validate_one(self, df: pd.DataFrame):\n",
    "\n",
    "        X_train, X_test, y_train, y_test = self.__train_test_split(df)\n",
    "\n",
    "        stepwise_fit = pm.auto_arima(y_train,\n",
    "                                     start_p=0, start_q=0,\n",
    "                                     max_p=3, max_q=3, max_Q=3, max_P=3,\n",
    "                                     d=0, D=1, m=7,\n",
    "                                     X=None,\n",
    "                                     seasonal=True, trace=True, stepwise=True)\n",
    "\n",
    "        return stepwise_fit\n",
    "\n",
    "    def __train_test_split(self, df: pd.DataFrame):\n",
    "        # take only needed column \n",
    "        df = df[[self.col_name]]\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            df.drop(columns=[self.col_name]),\n",
    "            df[[self.col_name]],\n",
    "            test_size=self.test_size,\n",
    "            shuffle=False\n",
    "        )\n",
    "\n",
    "        return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm \n",
    "\n",
    "class CreateDailyForecasts:\n",
    "\n",
    "    columns = [\"avg_power_demand_W\", \"energy_demand_kWh\", \"peak_power_W\"]\n",
    "\n",
    "    def __init__():\n",
    "        pass \n",
    "    \n",
    "    @classmethod\n",
    "    def run_daily_forecast(cls, df, best_params: dict):\n",
    "\n",
    "        existing_forecasts = pd.read_csv(\"forecastdata/dailyforecasts.csv\", index_col=\"time\", parse_dates=True)\n",
    "        new_forecasts = pd.DataFrame()\n",
    "\n",
    "        for column in cls.columns:\n",
    "            \n",
    "            # train on ALL available data\n",
    "            train = df[[column]].copy() \n",
    "            # create ARIMA model \n",
    "            best_model_arima = sm.tsa.arima.ARIMA(train, order=(1,0,0), seasonal_order=(1,1,1,7)).fit()\n",
    "            # forecast on day ahead, convert to a dataframe\n",
    "            one_column_forecast = best_model_arima.forecast()\n",
    "            one_column_forecast = pd.DataFrame(one_column_forecast, columns=[column+'_predictions']) \n",
    "            new_forecasts = pd.concat([new_forecasts, one_column_forecast], axis=1)\n",
    "\n",
    "        # append new forecasts existing set of forecasts\n",
    "        forecasts = pd.concat([existing_forecasts, new_forecasts], axis=0)\n",
    "\n",
    "        return forecasts\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def save_empty_prediction_df():\n",
    "        empty_df = pd.DataFrame(columns=[\"avg_power_demand_W_predictions\", \"energy_demand_kWh_predictions\", \"peak_power_W_predictions\"], index=pd.Index([], name=\"time\"))\n",
    "        empty_df.to_csv(\"forecastdata/dailyforecasts.csv\")\n",
    "        return empty_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c08b048",
   "metadata": {},
   "outputs": [],
   "source": [
    "hourlydemand = pd.read_csv(\"data/hourlydemand.csv\", index_col=\"time\", parse_dates=True)\n",
    "dailydemand = pd.read_csv(\"data/dailydemand.csv\", index_col=\"time\", parse_dates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b0d5c14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 336 candidates, totalling 336 fits\n",
      "Fitting 1 folds for each of 336 candidates, totalling 336 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'energy_demand_kWh': {'best_depth': 57, 'best_n_neighbors': 25},\n",
       " 'peak_power_W': {'best_depth': 57, 'best_n_neighbors': 23},\n",
       " 'avg_power_demand_W': {'best_depth': 57, 'best_n_neighbors': 25}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = HourlyCrossValidator(max_neighbors=25, max_depth=60).cross_validate(hourlydemand)\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f71772f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing stepwise search to minimize aic\n",
      " ARIMA(0,0,0)(1,1,1)[7] intercept   : AIC=7268.150, Time=1.03 sec\n",
      " ARIMA(0,0,0)(0,1,0)[7] intercept   : AIC=7687.579, Time=0.05 sec\n",
      " ARIMA(1,0,0)(1,1,0)[7] intercept   : AIC=7472.624, Time=0.38 sec\n",
      " ARIMA(0,0,1)(0,1,1)[7] intercept   : AIC=7269.656, Time=0.65 sec\n",
      " ARIMA(0,0,0)(0,1,0)[7]             : AIC=7685.712, Time=0.03 sec\n",
      " ARIMA(0,0,0)(0,1,1)[7] intercept   : AIC=7269.425, Time=0.31 sec\n",
      " ARIMA(0,0,0)(1,1,0)[7] intercept   : AIC=7471.491, Time=0.36 sec\n",
      " ARIMA(0,0,0)(2,1,1)[7] intercept   : AIC=7269.365, Time=1.41 sec\n",
      " ARIMA(0,0,0)(1,1,2)[7] intercept   : AIC=7269.558, Time=0.91 sec\n",
      " ARIMA(0,0,0)(0,1,2)[7] intercept   : AIC=7267.918, Time=0.64 sec\n",
      " ARIMA(0,0,0)(0,1,3)[7] intercept   : AIC=7269.458, Time=1.13 sec\n",
      " ARIMA(0,0,0)(1,1,3)[7] intercept   : AIC=7270.303, Time=2.30 sec\n",
      " ARIMA(1,0,0)(0,1,2)[7] intercept   : AIC=7266.974, Time=0.92 sec\n",
      " ARIMA(1,0,0)(0,1,1)[7] intercept   : AIC=7269.010, Time=0.58 sec\n",
      " ARIMA(1,0,0)(1,1,2)[7] intercept   : AIC=7268.702, Time=1.31 sec\n",
      " ARIMA(1,0,0)(0,1,3)[7] intercept   : AIC=7268.620, Time=1.35 sec\n",
      " ARIMA(1,0,0)(1,1,1)[7] intercept   : AIC=7267.220, Time=1.13 sec\n",
      " ARIMA(1,0,0)(1,1,3)[7] intercept   : AIC=7269.451, Time=3.82 sec\n",
      " ARIMA(2,0,0)(0,1,2)[7] intercept   : AIC=7246.959, Time=0.99 sec\n",
      " ARIMA(2,0,0)(0,1,1)[7] intercept   : AIC=7248.242, Time=0.90 sec\n",
      " ARIMA(2,0,0)(1,1,2)[7] intercept   : AIC=7248.949, Time=2.10 sec\n",
      " ARIMA(2,0,0)(0,1,3)[7] intercept   : AIC=7248.940, Time=1.70 sec\n",
      " ARIMA(2,0,0)(1,1,1)[7] intercept   : AIC=7247.033, Time=1.11 sec\n",
      " ARIMA(2,0,0)(1,1,3)[7] intercept   : AIC=7249.983, Time=3.01 sec\n",
      " ARIMA(3,0,0)(0,1,2)[7] intercept   : AIC=7246.594, Time=1.39 sec\n",
      " ARIMA(3,0,0)(0,1,1)[7] intercept   : AIC=7247.942, Time=0.97 sec\n",
      " ARIMA(3,0,0)(1,1,2)[7] intercept   : AIC=7248.587, Time=2.11 sec\n",
      " ARIMA(3,0,0)(0,1,3)[7] intercept   : AIC=7248.580, Time=2.19 sec\n",
      " ARIMA(3,0,0)(1,1,1)[7] intercept   : AIC=7246.666, Time=1.16 sec\n",
      " ARIMA(3,0,0)(1,1,3)[7] intercept   : AIC=7249.699, Time=3.84 sec\n",
      " ARIMA(3,0,1)(0,1,2)[7] intercept   : AIC=7244.805, Time=2.47 sec\n",
      " ARIMA(3,0,1)(0,1,1)[7] intercept   : AIC=7247.409, Time=1.51 sec\n",
      " ARIMA(3,0,1)(1,1,2)[7] intercept   : AIC=7246.800, Time=3.26 sec\n",
      " ARIMA(3,0,1)(0,1,3)[7] intercept   : AIC=7246.796, Time=4.87 sec\n",
      " ARIMA(3,0,1)(1,1,1)[7] intercept   : AIC=7244.919, Time=1.94 sec\n",
      " ARIMA(3,0,1)(1,1,3)[7] intercept   : AIC=7247.849, Time=4.39 sec\n",
      " ARIMA(2,0,1)(0,1,2)[7] intercept   : AIC=7243.281, Time=2.47 sec\n",
      " ARIMA(2,0,1)(0,1,1)[7] intercept   : AIC=7245.588, Time=1.63 sec\n",
      " ARIMA(2,0,1)(1,1,2)[7] intercept   : AIC=7245.476, Time=3.43 sec\n",
      " ARIMA(2,0,1)(0,1,3)[7] intercept   : AIC=7245.336, Time=5.04 sec\n",
      " ARIMA(2,0,1)(1,1,1)[7] intercept   : AIC=7243.457, Time=2.31 sec\n",
      " ARIMA(2,0,1)(1,1,3)[7] intercept   : AIC=7247.281, Time=4.60 sec\n",
      " ARIMA(1,0,1)(0,1,2)[7] intercept   : AIC=7248.000, Time=2.44 sec\n",
      " ARIMA(2,0,2)(0,1,2)[7] intercept   : AIC=7263.498, Time=3.70 sec\n",
      " ARIMA(1,0,2)(0,1,2)[7] intercept   : AIC=7250.149, Time=2.88 sec\n",
      " ARIMA(3,0,2)(0,1,2)[7] intercept   : AIC=7262.768, Time=3.42 sec\n",
      " ARIMA(2,0,1)(0,1,2)[7]             : AIC=7244.681, Time=2.23 sec\n",
      "\n",
      "Best model:  ARIMA(2,0,1)(0,1,2)[7] intercept\n",
      "Total fit time: 92.404 seconds\n",
      "Performing stepwise search to minimize aic\n",
      " ARIMA(0,0,0)(1,1,1)[7] intercept   : AIC=14243.468, Time=1.13 sec\n",
      " ARIMA(0,0,0)(0,1,0)[7] intercept   : AIC=14675.646, Time=0.04 sec\n",
      " ARIMA(1,0,0)(1,1,0)[7] intercept   : AIC=14481.487, Time=0.26 sec\n",
      " ARIMA(0,0,1)(0,1,1)[7] intercept   : AIC=14244.656, Time=0.80 sec\n",
      " ARIMA(0,0,0)(0,1,0)[7]             : AIC=14673.837, Time=0.02 sec\n",
      " ARIMA(0,0,0)(0,1,1)[7] intercept   : AIC=14245.415, Time=0.78 sec\n",
      " ARIMA(0,0,0)(1,1,0)[7] intercept   : AIC=14484.108, Time=0.15 sec\n",
      " ARIMA(0,0,0)(2,1,1)[7] intercept   : AIC=14359.142, Time=0.50 sec\n",
      " ARIMA(0,0,0)(1,1,2)[7] intercept   : AIC=inf, Time=2.77 sec\n",
      " ARIMA(0,0,0)(0,1,2)[7] intercept   : AIC=14357.447, Time=0.30 sec\n",
      " ARIMA(0,0,0)(2,1,0)[7] intercept   : AIC=14414.550, Time=0.52 sec\n",
      " ARIMA(0,0,0)(2,1,2)[7] intercept   : AIC=14262.200, Time=2.25 sec\n",
      " ARIMA(1,0,0)(1,1,1)[7] intercept   : AIC=14235.445, Time=1.44 sec\n",
      " ARIMA(1,0,0)(0,1,1)[7] intercept   : AIC=14237.948, Time=1.12 sec\n",
      " ARIMA(1,0,0)(2,1,1)[7] intercept   : AIC=14354.990, Time=0.61 sec\n",
      " ARIMA(1,0,0)(1,1,2)[7] intercept   : AIC=14355.321, Time=0.71 sec\n",
      " ARIMA(1,0,0)(0,1,0)[7] intercept   : AIC=14674.911, Time=0.06 sec\n",
      " ARIMA(1,0,0)(0,1,2)[7] intercept   : AIC=14234.855, Time=1.90 sec\n",
      " ARIMA(1,0,0)(0,1,3)[7] intercept   : AIC=14355.308, Time=0.73 sec\n",
      " ARIMA(1,0,0)(1,1,3)[7] intercept   : AIC=14356.575, Time=1.27 sec\n",
      " ARIMA(2,0,0)(0,1,2)[7] intercept   : AIC=14342.971, Time=0.57 sec\n",
      " ARIMA(1,0,1)(0,1,2)[7] intercept   : AIC=14280.465, Time=2.23 sec\n",
      " ARIMA(0,0,1)(0,1,2)[7] intercept   : AIC=14237.153, Time=1.86 sec\n",
      " ARIMA(2,0,1)(0,1,2)[7] intercept   : AIC=14344.808, Time=1.08 sec\n",
      " ARIMA(1,0,0)(0,1,2)[7]             : AIC=14354.848, Time=0.33 sec\n",
      "\n",
      "Best model:  ARIMA(1,0,0)(0,1,2)[7] intercept\n",
      "Total fit time: 23.455 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'energy_demand_kWh': ARIMA(order=(2, 0, 1), scoring_args={}, seasonal_order=(0, 1, 2, 7),\n",
       "       suppress_warnings=True),\n",
       " 'peak_power_W': ARIMA(order=(1, 0, 0), scoring_args={}, seasonal_order=(0, 1, 2, 7),\n",
       "       suppress_warnings=True),\n",
       " 'avg_power_demand_W': ARIMA(order=(2, 0, 1), scoring_args={}, seasonal_order=(0, 1, 2, 7),\n",
       "       suppress_warnings=True)}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = DailyCrossValidator.cross_validate(dailydemand)\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ca4a98b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'energy_demand_kWh': {'order': (2, 0, 1), 'seasonal_order': (0, 1, 2, 7)},\n",
       " 'peak_power_W': {'order': (1, 0, 0), 'seasonal_order': (0, 1, 2, 7)},\n",
       " 'avg_power_demand_W': {'order': (2, 0, 1), 'seasonal_order': (0, 1, 2, 7)}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{'energy_demand_kWh': {\"order\":(2, 0, 1), \"seasonal_order\":(0, 1, 2, 7)},\n",
    " 'peak_power_W': {\"order\":(1, 0, 0), \"seasonal_order\":(0, 1, 2, 7)},\n",
    " 'avg_power_demand_W': {\"order\":(2, 0, 1), \"seasonal_order\":(0, 1, 2, 7)}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e719b537",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'energy_demand_kWh': {'best_depth': 57, 'best_n_neighbors': 25},\n",
       " 'peak_power_W': {'best_depth': 57, 'best_n_neighbors': 23},\n",
       " 'avg_power_demand_W': {'best_depth': 57, 'best_n_neighbors': 25}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{'energy_demand_kWh': {'best_depth': 57, 'best_n_neighbors': 25},\n",
    " 'peak_power_W': {'best_depth': 57, 'best_n_neighbors': 23},\n",
    " 'avg_power_demand_W': {'best_depth': 57, 'best_n_neighbors': 25}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd703b4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "475dd3d8de922d6629668699edf7da91807dd0e731d2ca4abf0ed1b52cb8d54e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
